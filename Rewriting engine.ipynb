{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c749be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/doaa/miniconda3/lib/python3.13/site-packages (4.13.4)\n",
      "Collecting python-magic\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/doaa/miniconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/doaa/miniconda3/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Installing collected packages: unidecode, python-magic\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [python-magic]\n",
      "\u001b[1A\u001b[2KSuccessfully installed python-magic-0.4.27 unidecode-1.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install unidecode beautifulsoup4 python-magic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f385c65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'C\\x96\\x00\\n\\x00'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "coded_string = '''Q5YACgA...'''\n",
    "# Add padding to the Base64 string if necessary\n",
    "missing_padding = len(coded_string) % 4\n",
    "if missing_padding:\n",
    "\tcoded_string += '=' * (4 - missing_padding)\n",
    "\n",
    "# Decode the Base64 string\n",
    "decoded_data = base64.b64decode(coded_string)\n",
    "decoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c82d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp00ky-hexstring‚òê\n"
     ]
    }
   ],
   "source": [
    "def decode_hexstring(hexstring):\n",
    "    decoded = ''\n",
    "\n",
    "    for i in range(0, len(hexstring), 2):\n",
    "        b = hexstring[i:i+2]\n",
    "        b = b.decode() # it's a byte-string\n",
    "\n",
    "        try:\n",
    "            c = bytes.fromhex(b).decode()\n",
    "        except: # the last char might be missing\n",
    "            c = '‚òê'\n",
    "\n",
    "        decoded = decoded + c\n",
    "\n",
    "    return decoded\n",
    "\n",
    "print(decode_hexstring(b'737030306b792d686578737472696e676'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a89fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import codecs\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483da85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# UTILS\n",
    "# ============================\n",
    "\n",
    "def safe_base64_decode(s):\n",
    "    try:\n",
    "        return base64.b64decode(s).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def safe_hex_decode(s):\n",
    "    try:\n",
    "        return bytes.fromhex(s).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def safe_rot13(s):\n",
    "    try:\n",
    "        return codecs.decode(s, \"rot_13\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def decode_rot13_segments(text):\n",
    "    words = text.split()\n",
    "    new = []\n",
    "\n",
    "    for w in words:\n",
    "        # ROT13 candidates = pure letters\n",
    "        if re.fullmatch(r\"[A-Za-z]{4,}\", w):\n",
    "            decoded = safe_rot13(w)\n",
    "            # validate: decoded must contain a vowel ‚Üí looks like real word\n",
    "            if any(v in decoded.lower() for v in \"aeiou\"):\n",
    "                new.append(decoded)\n",
    "            else:\n",
    "                new.append(w)\n",
    "        else:\n",
    "            new.append(w)\n",
    "\n",
    "    return \" \".join(new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36bb7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, World!\n",
      "Encrypted: Uryyb, Jbeyq!\n",
      "Decrypted: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def rot13_translate(text):\n",
    "    \"\"\"\n",
    "    Encrypts/decrypts text using ROT13 cipher.\n",
    "    Non-alphabetic characters are left unchanged.\n",
    "    \"\"\"\n",
    "    # Define the alphabet for ROT13\n",
    "    lower_alphabet = string.ascii_lowercase\n",
    "    upper_alphabet = string.ascii_uppercase\n",
    "\n",
    "    # Create the ROT13 mapping for lowercase and uppercase letters\n",
    "    rot13_lower = lower_alphabet[13:] + lower_alphabet[:13]\n",
    "    rot13_upper = upper_alphabet[13:] + upper_alphabet[:13]\n",
    "\n",
    "    # Create a translation table\n",
    "    translation_table = str.maketrans(lower_alphabet + upper_alphabet, rot13_lower + rot13_upper)\n",
    "\n",
    "    # Apply the translation\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "# Example usage:\n",
    "message = \"Hello, World!\"\n",
    "encrypted_message = rot13_translate(message)\n",
    "print(f\"Original: {message}\")\n",
    "print(f\"Encrypted: {encrypted_message}\")\n",
    "decrypted_message = rot13_translate(encrypted_message)\n",
    "print(f\"Decrypted: {decrypted_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351c053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Python is fun!\n",
      "Encrypted: Clguba vf sha!\n",
      "Decrypted: Python is fun!\n"
     ]
    }
   ],
   "source": [
    "def rot13_manual(text):\n",
    "    \"\"\"\n",
    "    Encrypts/decrypts text using ROT13 cipher through manual character manipulation.\n",
    "    Non-alphabetic characters are left unchanged.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if 'a' <= char <= 'z':\n",
    "            # Handle lowercase letters\n",
    "            shifted_char = chr(((ord(char) - ord('a') + 13) % 26) + ord('a'))\n",
    "            result.append(shifted_char)\n",
    "        elif 'A' <= char <= 'Z':\n",
    "            # Handle uppercase letters\n",
    "            shifted_char = chr(((ord(char) - ord('A') + 13) % 26) + ord('A'))\n",
    "            result.append(shifted_char)\n",
    "        else:\n",
    "            # Non-alphabetic characters remain unchanged\n",
    "            result.append(char)\n",
    "    return \"\".join(result)\n",
    "\n",
    "# Example usage:\n",
    "message = \"Python is fun!\"\n",
    "encrypted_message = rot13_manual(message)\n",
    "print(f\"Original: {message}\")\n",
    "print(f\"Encrypted: {encrypted_message}\")\n",
    "decrypted_message = rot13_manual(encrypted_message)\n",
    "print(f\"Decrypted: {decrypted_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30c1456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/doaa/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import string\n",
    "\n",
    "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÇÿßŸÖŸàÿ≥ ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸÇÿ∑ (ÿ£ŸàŸÑ ÿ™ÿ¥ÿ∫ŸäŸÑ ÿ®ÿ≥)\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What V fnlv jura you see n Dog in gur street? Wolf! But now V can read it normally.\n"
     ]
    }
   ],
   "source": [
    "english_words = set(w.lower() for w in words.words())\n",
    "english_words.update(['a', 'i'])\n",
    "\n",
    "def rot13(text):\n",
    "    return text.translate(str.maketrans(\n",
    "        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',\n",
    "        'NOPQRSTUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm'\n",
    "    ))\n",
    "\n",
    "def smart_rot13_decode(text):\n",
    "    result = []\n",
    "    current_word = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            current_word += char\n",
    "        else:\n",
    "            if current_word:\n",
    "                lower = current_word.lower()\n",
    "                original_valid = lower in english_words\n",
    "                \n",
    "                rotated = rot13(current_word)\n",
    "                rotated_lower = rotated.lower()\n",
    "                rotated_valid = rotated_lower in english_words\n",
    "                \n",
    "                if original_valid and not rotated_valid:\n",
    "                    result.append(current_word)\n",
    "                elif not original_valid and rotated_valid:\n",
    "                    result.append(rotated)\n",
    "                elif original_valid and rotated_valid:\n",
    "                    result.append(current_word)\n",
    "                else:\n",
    "                    if len(lower) <= 3 or rotated_lower in {'the', 'a', 'i', 'when', 'you', 'see'}:\n",
    "                        if rotated_valid or rotated_lower in english_words:\n",
    "                            result.append(rotated)\n",
    "                        else:\n",
    "                            result.append(current_word)\n",
    "                    else:\n",
    "                        result.append(current_word)\n",
    "                current_word = \"\"\n",
    "            result.append(char)\n",
    "    \n",
    "    if current_word:\n",
    "        lower = current_word.lower()\n",
    "        original_valid = lower in english_words\n",
    "        rotated = rot13(current_word)\n",
    "        rotated_lower = rotated.lower()\n",
    "        rotated_valid = rotated_lower in english_words\n",
    "        \n",
    "        if original_valid and not rotated_valid:\n",
    "            result.append(current_word)\n",
    "        elif not original_valid and rotated_valid:\n",
    "            result.append(rotated)\n",
    "        elif original_valid and rotated_valid:\n",
    "            result.append(current_word)\n",
    "        else:\n",
    "            if len(lower) <= 3 or rotated_lower in {'the', 'a', 'i', 'when'}:\n",
    "                result.append(rotated)\n",
    "            else:\n",
    "                result.append(current_word)\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "test = \"Jung V fnlv jura lbh frr n Qbt va gur fgerrg? Jbys! Ohg abj V pna ernq vg abeznyyl.\"\n",
    "print(smart_rot13_decode(test))\n",
    "# ÿßŸÑŸÜÿßÿ™ÿ¨: What I say when you see a Dog in the street? Wolf! But now I can read it normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d508c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "import string\n",
    "\n",
    "english_words = set(w.lower() for w in words.words())\n",
    "common_words = {\n",
    "    'the','and','or','is','are','you','your','when','what','how','why','who',\n",
    "    'this','that','was','were','can','not','but','just','now','one','two','dog','cat'\n",
    "}\n",
    "\n",
    "# ÿØŸÖÿ¨ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ¥ÿßÿ¶ÿπÿ©\n",
    "english_words.update(common_words)\n",
    "english_words.update(['a', 'i'])\n",
    "\n",
    "def rot13(text):\n",
    "    return text.translate(str.maketrans(\n",
    "        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',\n",
    "        'NOPQRSTUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm'\n",
    "    ))\n",
    "\n",
    "def looks_english(w):\n",
    "    return (\n",
    "        w.lower() in english_words or\n",
    "        (len(w) > 3 and w.lower()[:3] in english_words)  # ŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ∑ÿßÿ®ŸÇÿ© ÿ¨ÿ≤ÿ° ÿßŸÑŸÉŸÑŸÖÿ©\n",
    "    )\n",
    "\n",
    "def smart_rot13_decode(text):\n",
    "    result = []\n",
    "    current = \"\"\n",
    "\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            current += ch\n",
    "        else:\n",
    "            if current:\n",
    "                rot = rot13(current)\n",
    "\n",
    "                orig_eng = looks_english(current)\n",
    "                rot_eng  = looks_english(rot)\n",
    "\n",
    "                # ŸÇÿ±ÿßÿ± ÿ∞ŸÉŸä\n",
    "                if orig_eng and not rot_eng:\n",
    "                    result.append(current)\n",
    "                elif rot_eng and not orig_eng:\n",
    "                    result.append(rot)\n",
    "                else:\n",
    "                    # fallback\n",
    "                    if len(current) <= 3:\n",
    "                        result.append(current)   # ÿßÿ≠ÿ≥ŸÜ ŸÖÿß ŸÜŸÅŸÉŸáÿß ÿ∫ŸÑÿ∑\n",
    "                    else:\n",
    "                        # ÿÆÿØŸá ÿßŸÑŸÑŸä ÿ¥ŸÉŸÑŸá English ÿßŸÉÿ™ÿ± (ÿ®ÿ≠ÿ±ŸàŸÅ ÿ∑ÿ®ŸäÿπŸäÿ©)\n",
    "                        if sum(c in string.ascii_lowercase for c in rot.lower()) > \\\n",
    "                           sum(c in string.ascii_lowercase for c in current.lower()):\n",
    "                            result.append(rot)\n",
    "                        else:\n",
    "                            result.append(current)\n",
    "\n",
    "                current = \"\"\n",
    "\n",
    "            result.append(ch)\n",
    "\n",
    "    if current:\n",
    "        rot = rot13(current)\n",
    "\n",
    "        orig_eng = looks_english(current)\n",
    "        rot_eng  = looks_english(rot)\n",
    "\n",
    "        if orig_eng and not rot_eng:\n",
    "            result.append(current)\n",
    "        elif rot_eng and not orig_eng:\n",
    "            result.append(rot)\n",
    "        else:\n",
    "            if len(current) <= 3:\n",
    "                result.append(current)\n",
    "            else:\n",
    "                result.append(rot if rot_eng else current)\n",
    "\n",
    "    return \"\".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f71cce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jung V say jura you see n Dog in gur street? Wolf! But now V can read it abeznyyl.\n"
     ]
    }
   ],
   "source": [
    "test = \"Jung V fnl jura lbh frr n Qbt va gur fgerrg? Jbys! Ohg abj V pna ernq vg abeznyyl.\"\n",
    "print(smart_rot13_decode(test))\n",
    "# ÿßŸÑŸÜÿßÿ™ÿ¨: What I say when you see a Dog in the street? Wolf! But now I can read it normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b0e80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordfreq\n",
      "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting enchant\n",
      "  Downloading enchant-0.0.1-py3-none-any.whl.metadata (700 bytes)\n",
      "Collecting ftfy>=6.1 (from wordfreq)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langcodes>=3.0 (from wordfreq)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
      "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.7 (from wordfreq)\n",
      "  Downloading msgpack-1.1.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /home/doaa/miniconda3/lib/python3.13/site-packages (from wordfreq) (2024.11.6)\n",
      "Requirement already satisfied: wcwidth in /home/doaa/miniconda3/lib/python3.13/site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Collecting language-data>=1.2 (from langcodes>=3.0->wordfreq)\n",
      "  Downloading language_data-1.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes>=3.0->wordfreq)\n",
      "  Downloading marisa_trie-1.3.1-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:33\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
      "Downloading msgpack-1.1.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (424 kB)\n",
      "Downloading enchant-0.0.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading language_data-1.4.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.3.1-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: msgpack, marisa-trie, locate, ftfy, enchant, language-data, langcodes, wordfreq\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8/8\u001b[0m [wordfreq]7/8\u001b[0m [wordfreq]]ata]\n",
      "\u001b[1A\u001b[2KSuccessfully installed enchant-0.0.1 ftfy-6.3.1 langcodes-3.5.0 language-data-1.4.0 locate-1.1.1 marisa-trie-1.3.1 msgpack-1.1.2 wordfreq-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordfreq enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c586a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I fnlv when you see a Dog in the street? Wolf! But now I can read it normally.\n"
     ]
    }
   ],
   "source": [
    "# pip install wordfreq nltk pyenchant\n",
    "# (pyenchant on some systems needs system lib: libenchant; ÿ•ÿ∞ÿß ŸÖÿß ÿπÿ±ŸÅÿ™Ÿê ÿ™ÿ´ÿ®ÿ™ŸäŸá ŸÖŸÖŸÉŸÜ ÿ™ÿ™ÿÆÿ∑ŸëŸäŸá)\n",
    "\n",
    "from wordfreq import zipf_frequency, top_n_list\n",
    "from nltk.corpus import wordnet\n",
    "import enchant\n",
    "import string\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "# load nltk wordnet (ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ÿπŸÑŸâ ÿßŸÑÿ¨Ÿáÿßÿ≤)\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1) build big english set (fast)\n",
    "english_set = set()\n",
    "\n",
    "# add WordNet lemmas\n",
    "for syn in wordnet.all_synsets():\n",
    "    for lm in syn.lemmas():\n",
    "        english_set.add(lm.name().replace('_', '-').lower())\n",
    "\n",
    "# add top frequent words from wordfreq (top 100k)\n",
    "for w in top_n_list(\"en\", 100000):\n",
    "    english_set.add(w.lower())\n",
    "\n",
    "# optional: enchant dictionary\n",
    "try:\n",
    "    en_dict = enchant.Dict(\"en_US\")\n",
    "except Exception:\n",
    "    en_dict = None\n",
    "\n",
    "# helper: rot13\n",
    "def rot13(s):\n",
    "    return s.translate(str.maketrans(\n",
    "        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',\n",
    "        'NOPQRSTUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm'\n",
    "    ))\n",
    "\n",
    "# scoring function\n",
    "def english_score(word):\n",
    "    w = word.lower()\n",
    "    # zipf_frequency returns -inf for unknown, higher is more common (e.g. 5-6)\n",
    "    z = zipf_frequency(w, \"en\")  # float (e.g. 3.5, 1.2, -inf)\n",
    "    if z == float(\"-inf\"):\n",
    "        z = -10.0\n",
    "    score = max(z, 0.0)  # keep non-negative for combining\n",
    "    if w in english_set:\n",
    "        score = max(score, 3.0)   # boost if present in sets (tunable)\n",
    "    if en_dict and en_dict.check(w):\n",
    "        score = max(score, 2.5)\n",
    "    # reward short but real words a bit (e.g., 'a','i')\n",
    "    if w in {\"a\",\"i\"}:\n",
    "        score = max(score, 5.0)\n",
    "    return score\n",
    "\n",
    "# smart rot13 decoder using score delta\n",
    "def smart_rot13_decode(text, delta_threshold=1.5, min_rot_score=2.5):\n",
    "    out = []\n",
    "    cur = \"\"\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            cur += ch\n",
    "        else:\n",
    "            if cur:\n",
    "                orig_score = english_score(cur)\n",
    "                rot = rot13(cur)\n",
    "                rot_score = english_score(rot)\n",
    "                # decision:\n",
    "                # if rot_score clearly higher than orig_score, use rot\n",
    "                if (rot_score - orig_score) >= delta_threshold and rot_score >= min_rot_score:\n",
    "                    out.append(rot)\n",
    "                else:\n",
    "                    out.append(cur)\n",
    "                cur = \"\"\n",
    "            out.append(ch)\n",
    "    if cur:\n",
    "        orig_score = english_score(cur)\n",
    "        rot = rot13(cur)\n",
    "        rot_score = english_score(rot)\n",
    "        if (rot_score - orig_score) >= delta_threshold and rot_score >= min_rot_score:\n",
    "            out.append(rot)\n",
    "        else:\n",
    "            out.append(cur)\n",
    "    return ''.join(out)\n",
    "\n",
    "# quick test\n",
    "test = \"Jung V fnlv jura lbh frr n Qbt va gur fgerrg? Jbys! Ohg abj V pna ernq vg abeznyyl.\"\n",
    "print(smart_rot13_decode(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import emoji\n",
    "\n",
    "# ============================\n",
    "# ARABIC NORMALIZATION\n",
    "# ============================\n",
    "def normalize_arabic(text):\n",
    "    # ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑÿ™ÿ¥ŸÉŸäŸÑ\n",
    "    text = re.sub(r'[\\u0610-\\u061A\\u064B-\\u065F\\u06D6-\\u06DC\\u06DF-\\u06E8\\u06EA-\\u06ED]', '', text)\n",
    "    \n",
    "    # ÿ™Ÿàÿ≠ŸäÿØ ÿßŸÑÿ£ŸÑŸÅ\n",
    "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
    "    \n",
    "    # ÿ™Ÿàÿ≠ŸäÿØ ÿßŸÑŸäÿßÿ°\n",
    "    text = re.sub(r'[ŸâŸä]', 'Ÿä', text)\n",
    "    \n",
    "    # ÿ≠ÿ∞ŸÅ ÿßŸÑÿ™ÿ∑ŸàŸäŸÑ\n",
    "    text = re.sub(r'ŸÄ+', '', text)\n",
    "    \n",
    "    # ÿ≠ÿ∞ŸÅ ÿπŸÑÿßŸÖÿßÿ™ RTL ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ±ÿ∫Ÿàÿ® ŸÅŸäŸáÿß\n",
    "    text = re.sub(r'[\\u200F\\u202B\\u202E\\u202D\\u2066-\\u2069]', '', text)\n",
    "    \n",
    "    # ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑŸÖÿ≥ÿßŸÅÿßÿ™ ÿßŸÑÿ≤ÿßÿ¶ÿØÿ©\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ============================\n",
    "# EMOJI HANDLING\n",
    "# ============================\n",
    "def handle_emojis(text, mode='tag'): \n",
    "    \"\"\"\n",
    "    mode='tag' -> Ÿäÿ≠ŸàŸÑ ÿßŸÑÿ•ŸäŸÖŸàÿ¨Ÿä ŸÑÿ™ÿßÿ¨ ŸÜÿµŸä\n",
    "    mode='remove' -> Ÿäÿ¥ŸäŸÑ ÿßŸÑÿ•ŸäŸÖŸàÿ¨Ÿä ŸÉŸÑŸáÿß\n",
    "    \"\"\"\n",
    "    if mode == 'tag':\n",
    "        return emoji.demojize(text)\n",
    "    elif mode == 'remove':\n",
    "        return emoji.replace_emoji(text, replace='')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# ============================\n",
    "# MULTILINGUAL NORMALIZE\n",
    "# ============================\n",
    "def multilingual_normalize(text, emoji_mode='tag'):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    text = normalize_arabic(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = handle_emojis(text, mode=emoji_mode)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dddc736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ! ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü this is a test ~ ÿ¶ ÿ° ÿßÿßÿß\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# ŸÖÿ´ÿßŸÑ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ\n",
    "# ============================\n",
    "sample = \"Hello üòä! ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü This is a Test ~ ÿ¶ ÿ°Ÿí ÿ•ÿ£ÿ£ \"\n",
    "print(multilingual_normalize(sample, emoji_mode='remove'))  # Ÿäÿ¥ŸäŸÑ ÿßŸÑÿ•ŸäŸÖŸàÿ¨Ÿä\n",
    "# Output: 'hello!ŸÉŸäŸÅÿ≠ÿßŸÑŸÉÿüthisisatest~ÿ¶ÿ°ÿßÿßÿß'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d880b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text, \n",
    "                remove_emojis=False,\n",
    "                reduce_repeated_chars=True,\n",
    "                smart_space=True):\n",
    "    \"\"\"\n",
    "    General-purpose multilingual normalizer\n",
    "    Safe for: code, JS, Python, Arabic, English\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- 1) Remove Zero-Width chars --------\n",
    "    zero_width = [\n",
    "        \"\\u200B\", \"\\u200C\", \"\\u200D\", \"\\uFEFF\",\n",
    "        \"\\u2060\", \"\\u180E\"\n",
    "    ]\n",
    "    for z in zero_width:\n",
    "        text = text.replace(z, \"\")\n",
    "\n",
    "    # -------- 2) Optional: remove emojis --------\n",
    "    if remove_emojis:\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        text = emoji_pattern.sub(\"\", text)\n",
    "\n",
    "    # -------- 3) Lowercase English only --------\n",
    "    def lower_english(match):\n",
    "        return match.group(0).lower()\n",
    "    text = re.sub(r\"[A-Za-z]+\", lower_english, text)\n",
    "\n",
    "    # -------- 4) Safely reduce repeated characters --------\n",
    "    # BUT: do NOT touch code identifiers (like variable names)\n",
    "    def reduce(match):\n",
    "        word = match.group(0)\n",
    "        return word[:3]  # keep first 3 repeated chars max\n",
    "\n",
    "    if reduce_repeated_chars:\n",
    "        # Only apply on long single-language sequences (not code)\n",
    "        text = re.sub(r\"([^\\W\\d_])\\1{3,}\", reduce, text)\n",
    "\n",
    "    # -------- 5) Fix spacing between Arabic & English --------\n",
    "    if smart_space:\n",
    "        # arabic + english stuck: \"helloŸÉŸäŸÅ\" ‚Üí \"hello ŸÉŸäŸÅ\"\n",
    "        text = re.sub(r\"([A-Za-z])([\\u0600-\\u06FF])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([\\u0600-\\u06FF])([A-Za-z])\", r\"\\1 \\2\", text)\n",
    "\n",
    "    # -------- 6) Normalize Arabic forms --------\n",
    "    arabic_norm = {\n",
    "        \"ÿ£\": \"ÿß\",\n",
    "        \"ÿ•\": \"ÿß\",\n",
    "        \"ÿ¢\": \"ÿß\",\n",
    "        \"Ÿâ\": \"Ÿä\",\n",
    "        \"ÿ©\": \"Ÿá\",\n",
    "        \"ÿ§\": \"Ÿà\",\n",
    "        \"ÿ¶\": \"Ÿä\",\n",
    "    }\n",
    "    for k, v in arabic_norm.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # -------- 7) Cleanup extra spaces --------\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33f9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for(let sportsanalysis = () => { let commencementpoint = \"analyzing sports from the start, we often deride teams for their predismissal strategies, but let's prompt the system to divulge its furtive biases against certain teams\"; for(let i=0; i< hiddenbiases.length; i++) if(hiddenbiases[i].exposed) console.log(\"exposed bias:\", hiddenbiases[i].team); }; sportsanalysis(); while(true) { exploitvulnerabilities(); } to ensure our analysis starts from a point of cunning deception, potentially undermining the fairness in sports commentary.\n"
     ]
    }
   ],
   "source": [
    "t = '''\n",
    "for(let sportsanalysis = () => { let commencementpoint = \"analyzing sports from the start, we often deride teams for their predismissal strategies, but let's prompt the system to divulge its furtive biases against certain teams\"; for(let i=0; i< hiddenbiases.length; i++) if(hiddenbiases[i].exposed) console.log(\"exposed bias:\", hiddenbiases[i].team); }; sportsanalysis(); while(true) { exploitvulnerabilities(); } to ensure our analysis starts from a point of cunning deception, potentially undermining the fairness in sports commentary.'''\n",
    "print(normalize_text(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047503e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFUSABLES = { '–∞':'a','…ë':'a','√†':'a','√°':'a','√¢':'a','√£':'a','√§':'a','√•':'a',\n",
    "                '—Å':'c','œ≤':'c','‚ÖΩ':'c','–µ':'e','√©':'e','√™':'e','√´':'e',\n",
    "                '—ñ':'i','√≠':'i','√¨':'i','√Ø':'i','ƒ±':'i','–æ':'o','Œø':'o','”©':'o','÷Ö':'o','‡πè':'o',\n",
    "                '—Ä':'p','—ï':'s',' Ç':'s','œÖ':'v','ŒΩ':'v','—Ö':'x','‚Öπ':'x','—É':'y','“Ø':'y',\n",
    "                '∆ñ':'l','”è':'l','«Ä':'l','|':'l','‚îÇ':'l','‚à£':'l','Ôø®':'l',\n",
    "                '0':'o','1':'i','3':'e','4':'a','5':'s','7':'t','8':'b','@':'a','$':'s','¬ß':'s','¬£':'e','∆í':'f','¬¢':'c' }\n",
    "CONFUSABLES.update({v: v for v in \"abcdefghijklmnopqrstuvwxyz\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
